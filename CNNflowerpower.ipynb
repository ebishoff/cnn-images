{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix B Python Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1: Here we have the code needed to create a Convolutional Neural Network for classifying a flower image dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we have all the packages needed to create our Neural Net\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import time\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 2: Here we upload the images and resize them to by 100 by 100 by 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function just takes the images and opens them, resizes them, \n",
    "#and puts them in an array\n",
    "def LoadDir(dirname):\n",
    "    imgs = []\n",
    "    for imgname in os.listdir(dirname):\n",
    "        img = Image.open(os.path.join(dirname, imgname))\n",
    "        img = img.resize([100, 100])\n",
    "        img = np.squeeze(np.array(img)[:, :, :])\n",
    "        imgs.append(img)\n",
    "    return np.array(imgs)\n",
    "#here I load the images \n",
    "daisy_imgs = LoadDir('C:\\\\Users\\\\ebish\\\\Documents\\\\data analysis 582\\\\flowers\\\\daisy')\n",
    "print(daisy_imgs.shape)\n",
    "dandelion_imgs = LoadDir('C:\\\\Users\\\\ebish\\\\Documents\\\\data analysis 582\\\\flowers\\\\dandelion')\n",
    "print(dandelion_imgs.shape)\n",
    "rose_imgs = LoadDir('C:\\\\Users\\\\ebish\\\\Documents\\\\data analysis 582\\\\flowers\\\\rose')\n",
    "print(rose_imgs.shape)\n",
    "sunflower_imgs = LoadDir('C:\\\\Users\\\\ebish\\\\Documents\\\\data analysis 582\\\\flowers\\\\sunflower')\n",
    "print(sunflower_imgs.shape)\n",
    "tulip_imgs = LoadDir('C:\\\\Users\\\\ebish\\\\Documents\\\\data analysis 582\\\\flowers\\\\tulip')\n",
    "print(tulip_imgs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 3: Here we create a plot of some of the images in the dataset just to get an idea of what we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the different images\n",
    "plt.subplot(1,5,1)\n",
    "plt.imshow(daisy_imgs[1])\n",
    "plt.axis('off')\n",
    "plt.subplot(1,5,2)\n",
    "plt.imshow(dandelion_imgs[1])\n",
    "plt.axis('off')\n",
    "plt.subplot(1,5,3)\n",
    "plt.imshow(rose_imgs[1])\n",
    "plt.axis('off')\n",
    "plt.subplot(1,5,4)\n",
    "plt.imshow(sunflower_imgs[1])\n",
    "plt.axis('off')\n",
    "plt.subplot(1,5,5)\n",
    "plt.imshow(tulip_imgs[8])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 4: We normalize the images and then create labels for the flowers and combine all of it together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the data\n",
    "daisy_imgs=daisy_imgs/255\n",
    "dandelion_imgs=dandelion_imgs/255\n",
    "rose_imgs=rose_imgs/255\n",
    "sunflower_imgs=sunflower_imgs/255\n",
    "tulip_imgs=tulip_imgs/255\n",
    "#creating the labels for the data by finding their lenght and then \n",
    "#creating an array that holds thier class as an integer\n",
    "total=len(daisy_imgs)+len(dandelion_imgs)+len(rose_imgs)+len(sunflower_imgs)+len(tulip_imgs)\n",
    "y=np.zeros(shape=(total,1))\n",
    "a=len(daisy_imgs)\n",
    "b=len(dandelion_imgs)\n",
    "c=len(rose_imgs)\n",
    "d=len(sunflower_imgs)\n",
    "e=len(tulip_imgs)\n",
    "y[a:a+b-1]=1; #dandelions\n",
    "y[a+b:a+b+c-1]=2; #rose\n",
    "y[a+b+c:a+b+c+d-1]=3; #sunflower\n",
    "y[a+b+c+d:a+b+c+d+e-1]=4; #tulip\n",
    "#combine all the flower data\n",
    "allflowers=np.vstack([daisy_imgs, dandelion_imgs, rose_imgs, sunflower_imgs,tulip_imgs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 5: Created the training, validation, and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_testval, y_train,y_testval=train_test_split(allflowers,y,test_size=.20)\n",
    "X_validate, X_test,  y_validate, y_test=train_test_split(X_testval, y_testval, test_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our nerual net will take the y dataset and turn in essetially into\n",
    "#one hot encoding\n",
    "y_train=to_categorical(y_train, num_classes=5)\n",
    "y_test=to_categorical(y_test, num_classes=5)\n",
    "y_validate=to_categorical(y_validate, num_classes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 6: Here we created an initial convolutional nerual net. We have 2 convoltuional layers with maxpooling layers followed after each one. Then we go straight to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial model has 2 convolutaional layers where our filter is (3,3)\n",
    "#with a RELU activation function. Two maxpooling layers that follow\n",
    "#each convolutional layer with a filter of the size (2,2)\n",
    "#No dense layer other than to compute the output with a softmax \n",
    "#activation function. Loss=categorical crossentropy with a Adam \n",
    "#optimizer.\n",
    "model1=Sequential()\n",
    "model1.add( Conv2D(64,(3,3), input_shape=X_train.shape[1:]) )\n",
    "model1.add(Activation(\"relu\"))\n",
    "model1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model1.add( Conv2D(64,(3,3) ))\n",
    "model1.add(Activation(\"relu\"))\n",
    "model1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model1.add(Flatten())\n",
    "\n",
    "model1.add(Dense(5))\n",
    "model1.add(Activation('softmax'))\n",
    "\n",
    "model1.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n",
    "model1.fit(X_train, y_train, batch_size=32, epochs=5, validation_split=.1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 7: Here we create the loop that tests out every combination of dense layer, nodes, and convolutional layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-48217d825df0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;31m#this connects with tensorboard and allows us to plot the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;31m#accuracy of each model that we iterate through\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mNAME\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"{}-conv-{}-nodes-{}-dense-{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mtensorboard\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'logs/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "#different testing values\n",
    "dense_layers=[0, 1, 2]\n",
    "layer_sizes=[32, 64, 128]\n",
    "conv_layers=[1,2,3]\n",
    "\n",
    "#for loop to try out the different models where it will try every\n",
    "#combination of dense_layers, layer_sizes, and conv_layers.\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            #this connects with tensorboard and allows us to plot the \n",
    "            #accuracy of each model that we iterate through\n",
    "            NAME=\"{}-conv-{}-nodes-{}-dense-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "            tensorboard=TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "            print(NAME)\n",
    "            model1=Sequential()\n",
    "            model1.add( Conv2D(layer_size,(3,3), input_shape=X_train.shape[1:]) )\n",
    "            model1.add(Activation(\"relu\"))\n",
    "            model1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "            for l in range(conv_layer-1):\n",
    "                model1.add( Conv2D(layer_size,(3,3) ))\n",
    "                model1.add(Activation(\"relu\"))\n",
    "                model1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "                \n",
    "            model1.add(Flatten())    \n",
    "            for l in range(dense_layer):\n",
    "                model1.add(Dense(layer_size))\n",
    "                model1.add(Activation(\"relu\"))\n",
    "\n",
    "            model1.add(Dense(5))\n",
    "            model1.add(Activation('softmax'))\n",
    "            #compile the model\n",
    "            model1.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])\n",
    "            #train the model with the given data \n",
    "            model1.fit(X_train, y_train, batch_size=32, epochs=5, validation_split=.1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 8: Here we prep to do data augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we do some data augmentation to help us have more training samples\n",
    "X_train_ch=X_train\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "d=datagen.fit(X_train_ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 9: Here we create our model1, model2, and model3 with newly added dropout and data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 1\n",
    "NAME=\"{}-conv-{}-nodes-{}-dense-{}-dropout-dgm\".format(3, 32, 0, int(time.time()))\n",
    "tensorboard=TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "print(NAME)\n",
    "model1=Sequential()\n",
    "model1.add( Conv2D(32,(3,3), input_shape=X_train.shape[1:]) )\n",
    "model1.add(Activation(\"relu\"))\n",
    "model1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "           \n",
    "model1.add( Conv2D(32,(3,3) ))\n",
    "model1.add(Activation(\"relu\"))\n",
    "model1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model1.add( Conv2D(32,(3,3) ))\n",
    "model1.add(Activation(\"relu\"))\n",
    "model1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "                \n",
    "model1.add(Flatten())\n",
    "model1.add(Dropout(0.2))\n",
    "\n",
    "model1.add(Dense(5))\n",
    "model1.add(Activation('softmax'))\n",
    "\n",
    "model1.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n",
    "model1.fit_generator(datagen.flow(X_train_ch,y_train, batch_size=32),\n",
    "                              epochs = 10, validation_data=(X_validate,y_validate), callbacks=[tensorboard])\n",
    "                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 2\n",
    "NAME=\"{}-conv-{}-nodes-{}-dense-{}-dropout-dgm2\".format(3, 64, 1, int(time.time()))\n",
    "tensorboard=TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "print(NAME)\n",
    "model2=Sequential()\n",
    "model2.add( Conv2D(64,(3,3), input_shape=X_train.shape[1:]) )\n",
    "model2.add(Activation(\"relu\"))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "           \n",
    "model2.add( Conv2D(64,(3,3) ))\n",
    "model2.add(Activation(\"relu\"))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model2.add( Conv2D(64,(3,3) ))\n",
    "model2.add(Activation(\"relu\"))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "                \n",
    "model2.add(Flatten())    \n",
    "model2.add(Dense(64))\n",
    "model2.add(Activation(\"relu\"))\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Dense(5))\n",
    "model2.add(Activation('softmax'))\n",
    "\n",
    "model2.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n",
    "model2.fit_generator(datagen.flow(X_train_ch,y_train, batch_size=32),\n",
    "                              epochs = 10, validation_data=(X_validate,y_validate), callbacks=[tensorboard])\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 3\n",
    "NAME=\"{}-conv-{}-nodes-{}-dense-{}-dropout-dgm2\".format(3, 64, 2, int(time.time()))\n",
    "tensorboard=TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "print(NAME)\n",
    "model3=Sequential()\n",
    "model3.add( Conv2D(64,(3,3), input_shape=X_train.shape[1:]) )\n",
    "model3.add(Activation(\"relu\"))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "           \n",
    "model3.add( Conv2D(64,(3,3) ))\n",
    "model3.add(Activation(\"relu\"))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model3.add( Conv2D(64,(3,3) ))\n",
    "model3.add(Activation(\"relu\"))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "                \n",
    "model3.add(Flatten())    \n",
    "model3.add(Dense(64))\n",
    "model3.add(Activation(\"relu\"))\n",
    "\n",
    "model3.add(Dense(64))\n",
    "model3.add(Activation(\"relu\"))\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "model3.add(Dense(5))\n",
    "model3.add(Activation('softmax'))\n",
    "\n",
    "model3.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n",
    "model3.fit_generator(datagen.flow(X_train_ch,y_train, batch_size=32),\n",
    "                              epochs = 10, validation_data=(X_validate,y_validate), callbacks=[tensorboard])\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 10: Here we test different acitvaiton functions on our best model from Section 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing different activaiton functions \n",
    "act=[\"sigmoid\",\"tanh\", \"relu\"]\n",
    "\n",
    "\n",
    "for act_layer in act:\n",
    "    NAME=\"{}-conv-{}-nodes-{}-dense-{}-dropout-{}-activation-dgm2\".format(3, 64, 2,act_layer, int(time.time()))\n",
    "    tensorboard=TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "    print(NAME)\n",
    "    model4=Sequential()\n",
    "    model4.add( Conv2D(64,(3,3), input_shape=X_train.shape[1:]) )\n",
    "    model4.add(Activation(act_layer))\n",
    "    model4.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "    model4.add( Conv2D(64,(3,3) ))\n",
    "    model4.add(Activation(act_layer))\n",
    "    model4.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model4.add( Conv2D(64,(3,3) ))\n",
    "    model4.add(Activation(act_layer))\n",
    "    model4.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model4.add(Flatten())    \n",
    "    model4.add(Dense(64))\n",
    "    model4.add(Activation(act_layer))\n",
    "    model4.add(Dropout(0.2))\n",
    "    \n",
    "    model4.add(Dense(64))\n",
    "    model4.add(Activation(act_layer))\n",
    "   \n",
    "    \n",
    "    model4.add(Dense(5))\n",
    "    model4.add(Activation('softmax'))\n",
    "\n",
    "    model4.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n",
    "    model4.fit_generator(datagen.flow(X_train_ch,y_train, batch_size=32),\n",
    "                                  epochs = 5, validation_data=(X_validate,y_validate), callbacks=[tensorboard])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 11: now test between our two final models with 3 convolutional layers vs 4 convolutional layers. We test to see if more epochs will increase the overall accuracy on our validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME=\"{}-conv-{}-nodes-{}-dense-{}-dropout-{}-activation-dgm2\".format(3, 64, 2,\"relu\", int(time.time()))\n",
    "tensorboard=TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "print(NAME)\n",
    "model6=Sequential()\n",
    "model6.add( Conv2D(64,(3,3), input_shape=X_train.shape[1:]) )\n",
    "model6.add(Activation(\"relu\"))\n",
    "model6.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model6.add( Conv2D(64,(3,3) ))\n",
    "model6.add(Activation(\"relu\"))\n",
    "model6.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model6.add( Conv2D(64,(3,3) ))\n",
    "model6.add(Activation(\"relu\"))\n",
    "model6.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model6.add(Flatten())    \n",
    "model6.add(Dense(64))\n",
    "model6.add(Activation(\"relu\"))\n",
    "model6.add(Dropout(0.2))\n",
    "    \n",
    "model6.add(Dense(64))\n",
    "model6.add(Activation(\"relu\"))\n",
    "   \n",
    "    \n",
    "model6.add(Dense(5))\n",
    "model6.add(Activation('softmax'))\n",
    "\n",
    "model6.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n",
    "model6.fit_generator(datagen.flow(X_train_ch,y_train, batch_size=32),\n",
    "                                  epochs = 25, validation_data=(X_validate,y_validate), callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME=\"{}-conv-{}-nodes-{}-dense-{}-dropout-{}-activation-dgm2\".format(4, 64, 2,act_layer, int(time.time()))\n",
    "tensorboard=TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "print(NAME)\n",
    "model7=Sequential()\n",
    "model7.add( Conv2D(64,(3,3), input_shape=X_train.shape[1:]) )\n",
    "model7.add(Activation(act_layer))\n",
    "model7.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model7.add( Conv2D(64,(3,3) ))\n",
    "model7.add(Activation(act_layer))\n",
    "model7.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model7.add( Conv2D(64,(3,3) ))\n",
    "model7.add(Activation(act_layer))\n",
    "model7.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model7.add( Conv2D(64,(3,3) ))\n",
    "model7.add(Activation(\"relu\"))\n",
    "model7.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model7.add(Flatten())    \n",
    "model7.add(Dense(64))\n",
    "model7.add(Activation(act_layer))\n",
    "model7.add(Dropout(0.2))\n",
    "    \n",
    "model7.add(Dense(64))\n",
    "model7.add(Activation(act_layer))\n",
    "   \n",
    "    \n",
    "model7.add(Dense(5))\n",
    "model7.add(Activation('softmax'))\n",
    "\n",
    "model7.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n",
    "model7.fit_generator(datagen.flow(X_train_ch,y_train, batch_size=32),\n",
    "                                  epochs = 25, validation_data=(X_validate,y_validate), callbacks=[tensorboard])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 12:We have finally picked a final model. Model 7 is the best that we have. Here we create a confusion matrix to try to understand the misclassification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_model7=model7.predict_classes(X_test)\n",
    "y_test_returned=np.argmax(y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function creates a confusion matrix. \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.rcParams.update({'font.size':22})\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.rcParams.update({'font.size':32})\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test_returned, y_pred_test_model7);\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=['daisy','dandelion','rose','sunflower','tulip'],\n",
    "                      title='Confusion matrix, with normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 13: Create the classifcation chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_returned, y_pred_test_model7,target_names=['daisy','dandelion','rose','sunflower','tulip']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 14: This was my way of finding the misclassified images and then plotting them to try to better understand what was going on.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_test_model7[10:20])\n",
    "print(y_test_returned[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots different images of the flowers from the test data to be able \n",
    "#to try to understand what is going on with the data \n",
    "#Tulip to rose 4 to 2\n",
    "plt.rcParams.update({'font.size':8})\n",
    "first=plt.subplot(1,4,1)\n",
    "first.title.set_text('Predicted:Tulip True Label:Rose')\n",
    "plt.imshow(X_test[19])\n",
    "plt.axis('off')\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(X_test[20])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(X_test[205])\n",
    "plt.axis('off')\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(X_test[242])\n",
    "plt.axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opposite Rose to tulip 2 to 4\n",
    "\n",
    "second=plt.subplot(1,4,1)\n",
    "second.title.set_text('Predicted:Rose vs. True Label:Tulip')\n",
    "plt.imshow(X_test[27])\n",
    "plt.axis('off')\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(X_test[114])\n",
    "plt.axis('off')\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(X_test[221])\n",
    "plt.axis('off')\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(X_test[276])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sunflower to rose 3 to 2\n",
    "c=plt.subplot(2,4,5)\n",
    "c.title.set_text('P: Sunflower T: Rose')\n",
    "plt.imshow(X_test[120])\n",
    "plt.axis('off')\n",
    "plt.subplot(2,4,6)\n",
    "plt.imshow(X_test[201])\n",
    "plt.axis('off')\n",
    "#sunflower to tulip 3 to 4\n",
    "d=plt.subplot(2,4,7)\n",
    "d.title.set_text('P: Sunflower T: Tulip')\n",
    "plt.imshow(X_test[25])\n",
    "plt.axis('off')\n",
    "plt.subplot(2,4,8)\n",
    "plt.imshow(X_test[15])\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
